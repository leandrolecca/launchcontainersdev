########################
# Instruction to set your lc_config.yaml
# This is the config file for running the launchcontainers.py
# This file should be copied from garikoitz/launchcontainer/example_configs to your local dir: ~/PROJECT/nifti with another file: subSES.txt 
########################
# Structure:

# There are three dictionaries in this yaml: config, container_options, and host_options
# The config dictionary defines the basic setting of the launchcontainers.py it will specify your project path, what container you are using 
# and what HPC server you are using.  

# The container_options defines the parameters that are related to each container. It is correlated to the prepare mode when the program creates
# folder structures for you

# The host_options defines the parameters related to Python dask, a package to send jobs to the cluster.

# Detailed explanations are explained under each dictionary.
########################

config:
  # The parametes and their explanation are explained in the comments, we also gives you examples of each parameters. 
  # basedir: 
      # type: str
      # Base folder of your project where the dicom and nifti folder are. 
      # Remember to give the full path of the directory starting from /, do not use ~
      # If you are using public folders, go from generic public wihout using your path e.g. useing /bcbl/public instead of using /export/xxxx/public
  # containerdir:
      # type: str
      # Path to where all the singularity images are stored. For Docker, this can be empty
  
  # container:
      # type: str
      # The container name that you want to run, version and other details will be stored in container_options, you need to edit them also
  # analysis:
      # type: str or int
      # The name of your analysis, every analysis will have a folder under derivetives folder
  # force:
      # type: bool/ don't sensitive to capital letters True and true both okay
      # Options whether you will overwritte the existing file or not
      # This force command applies to symlinks and analyses, if it set to true, it will always overwrite everything
      # For PREPARE MODE
      # if file_exists and force: overwrite
      # if file_exists and not force: do nothing and print(say that the file existed but you kept it)
      # if not file_exists: we don't care about force, you add the file (the symlink)
  # host
      # type: str
      # The host where this will be run, see below for options
  basedir: /scratch/llecca/devtrajtract/DATA/BERTSOLARI
  containerdir: /scratch/llecca/containers
  container: "anatrois"  # VALID OPTIONS: anatrois, rtppreproc, rtp-pipeline, TODO: heudiconv, prfprepare, prfanalyze, prfresults, fmriprep
  analysis: "02"
  force: false
  # The host where this will be run, see below for options  
  host: DIPC
  # If true it will use sge or slumr, otherwise it just generates the "docker" or "singularity" commands
  # Good for testing, just launch for one subject, for example
  qsub: true
  # Set this to true to just show the text of what it is going to launch
  # If this is set to true, it will not launch anything, it will only throw the text with the commands that will be launch
  # For testing only
  # TODO: implement this option
  # note for tiger: in the sh you did for heudiconv, this would be as
  # echo $CMD
  # if text_only=false: eval $CMD
  # cmd="do this and this"
  # print(cmd)
  # if not text_only: subprocess.run(cmd,shell=True)
  text_only: true

container_options:
# Add the containers and options you want to run
  fmriprep:
    version: 
    space: ""
  anatrois:
    version: 4.5.2-7.3.2
    # If you have run FS previously, you can say true here and it will use the existing output
    pre_fs: false
    # If pre_fs is true, it will try to find it using the options below
    # Add the container name and versions used to create the pre_fs
    precontainerfs: anatrois_4.5.2-7.3.2
    # There can be more than one analysis, give the number of the analysis here
    preanalysisfs: "01"
    # It will find a zip file in the anatrois output, that starts with this string
    prefs_zipname: "anatrois"
    # These are optional input files. If there is none, leave it empty string
    # If it is empty, it will ignore it and will not create the input/folder
    annotfile: ""
    mniroizip: ""
  rtppreproc:
    version: 1.2.0-3.0.3
    # It checks if there is a reverse phase encoding acquisition
    # Old dcm2nixx will not create empty bvec and bval files if there was an acquisition with just b0-s
    # This code will create them automatically if they are not there
    # TODO: make it a function
    rpe: ture
    # Find where the input files are. It will take the T1 and the brainmask from here
    #
    #this thing was pretoolfs originally
    precontainerfs: anatrois_4.5.2-7.3.2
    preanalysisfs: "01"
  rtp-pipeline:
    version: 4.4.1
    # Find where the input files are. It will take the T1 and the brainmask from here        
    precontainerfs: anatrois_4.2.7-7.1.1
    preanalysisfs: "01"
    precontainerpp: rtppreproc_1.2.0-3.0.3
    preanalysispp: "01"
    # Porject level tractparams or individual tractparams can be used. 
    # If it is poject level, there should be one tractparams in the analysis-xx folder, and it will crate a symlink in every subjects input folder
    # and if the file is not there, it will throw an error. 
    # If the option is seet to false, then the program will need to check that the tractparams file is in the inptu fodler per every subject, otherwise it will fail
    # TODO: the checking system is not implemented, the createSymLink it is
    one_tractparams_per_analysis: true
  prfprepare:
    version: 
  prfanalyze:
    version: 

host_options:
    # Default BCBL
    BCBL:
      sin_ver: singularity/3.5.2
      maxwall: 10
      manager: sge
      name: "anatrois"
      # Dask worker options
      cores: 6                    # Total number of cores per job (it was core for BCBL)
      memory: 32G                # Total amount of memory per job (it was mem for BCBL)
      processes: 1                # Number of Python processes per job

      interface: lo             # Network interface to use like eth0 or ib0
      death-timeout: 100           # Number of seconds to wait if a worker can not find a scheduler
      local-directory: null       # Location of fast local storage like /scratch or $TMPDIR

      # SGE resource manager options
      #shebang: "#!/usr/bin/env bash"
      queue: long.q              # It was que in BCBL
      project: null
      walltime: 25:30:00'
      extra: []
      env-extra: []
      job-extra: []
      logdir: /export/home/llecca/public/LEPA/testlaunchcontainers/logdir
      resource-spec: null


    # Defaul DIPC
    DIPC:
      memory: 32G
      queue: regular
      cores: 24
      walltime: '22:00:00'
      sin_ver: Singularity/3.5.3-GCC-8.3.0
      manager: slurm
      system: scratch
      name: "anatrois"
    # maxwall: 5
      logdir: /scratch/llecca/devtrajtract/DATA/BERTSOLARI/logdir
      tmpdir: /scratch/llecca/tmp
    # Other
